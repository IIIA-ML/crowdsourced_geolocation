{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir('/home/yanis/crowd_geoloc') \n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, uniform\n",
    "import cmdstanpy as cmd\n",
    "import seaborn as sn\n",
    "from src.crowdgeoloc.one_d import sample_tokyo_latitudes, generate_annotators, annotate, \\\n",
    "FunctionNormalAnnotatorPopulation, SimpleNormalAnnotatorPopulation\n",
    "from src.crowdgeoloc.experiment import ActiveAnnotationContest, mean_location_norm_error, mean_sigma_norm_error,softmax_stable\n",
    "from src.crowdgeoloc.fixed import OneShotDirect,OneShotBayesian, OneShotConservative, OneShotConservative2, OneShotIterative, \\\n",
    "                                OneShotMean, KShot\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "n_annotators = 50\n",
    "redundancy = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06af774",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {#\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          #\"iterative\":OneShotIterative(), \n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          #\"conservative\":OneShotConservative(),\n",
    "          \"10shot\":KShot(greedyness=0.1)\n",
    "         }\n",
    "metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokyo latitude sampling\n",
    "def tok(n):\n",
    "    t=np.array(sample_tokyo_latitudes(n)) #we sample the n points\n",
    "    tok_norm=(t-np.min(t))/(np.max(t)-np.min(t)) #we normalize\n",
    "    return(tok_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to run multiple experiments and compute metrics\n",
    "\n",
    "def compare(models, exp, metrics, repeats):\n",
    "    results = []\n",
    "    duration_list=[]\n",
    "    for model_name, m in models.items():\n",
    "        #print(model_name)\n",
    "        result = {\"model\": model_name}\n",
    "        start_time = time.time()\n",
    "        for i in range(repeats):\n",
    "            exp.reset()\n",
    "            this_run = m.run(exp)\n",
    "            result[\"iteration\"] = i\n",
    "            for metric_name, metric in metrics.items():\n",
    "                result[\"metric\"] = metric_name\n",
    "                result[\"value\"] = metric(exp, this_run)\n",
    "                results.append(result.copy())\n",
    "        end_time=time.time()\n",
    "        duration=end_time-start_time\n",
    "        duration_list.append(duration)\n",
    "    return results,duration_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ab273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters are defined as a list of this shape:\n",
    "\n",
    "#param=[n_points,n_annotators,redundancy,sigma_distrib,point_distrib,model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(path):\n",
    "    # Load the pickled file\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an experiment for a pickle file, return list of params\n",
    "def load_experiment_setup(path):\n",
    "    # Load the pickled file\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return([data[\"nb_points\"],data[\"nb_annotators\"],data[\"redundancy\"],data[\"sigma_distrib\"],data[\"point_distrib\"],data[\"points\"],data[\"sigmas\"],data[\"random_seed\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a single experiments (one set of params) and get the true pos, true sigmas, and predicted pos, predicted sigmas\n",
    "def runexp(setup_file):\n",
    "    \n",
    "    '''\n",
    "    takes a str: setup file as argument and compare the methods based on this setup\n",
    "    '''\n",
    "\n",
    "    params=load_experiment_setup(setup_file) #open setup file\n",
    "    np.random.seed(params[7])\n",
    "    \n",
    "    models = {#\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          #\"iterative\":OneShotIterative(), \n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          #\"conservative\":OneShotConservative(),\n",
    "          \"10shot\":KShot(greedyness=0.1)\n",
    "         }\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "    \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2]) #choice of general parameters\n",
    "    sig_distr=params[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "    point_distr=params[4] #choice of point distrib\n",
    "    \n",
    "    if point_distr=='uniform':\n",
    "        point_distribution = uniform()\n",
    "        points = point_distribution.rvs(n_points)\n",
    "    else:\n",
    "        points =  tok(n_points)\n",
    "        \n",
    "    list_tru_sig=[]\n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    list_true_sig=[ann_set.annotators[k]._sigma for k in range(len(ann_set.annotators))]\n",
    "    \n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    list_sigm_pred=[]\n",
    "    list_point_pred=[]\n",
    "    for model_name, m in models.items():\n",
    "        print(model_name)\n",
    "        result = {\"model\": model_name}\n",
    "        exp.reset()\n",
    "        this_run = m.run(exp)\n",
    "        locations=this_run[\"locations\"]\n",
    "        sigmas=this_run[\"sigmas\"]\n",
    "        list_sigm_pred.append(sigmas)\n",
    "        list_point_pred.append(locations)\n",
    "        #for metric_name, metric in metrics.items():\n",
    "            #result[\"metric\"] = metric_name\n",
    "            #result[\"value\"] = metric(exp, this_run)\n",
    "            #results.append(result.copy())\n",
    "    return points,list_true_sig,list_point_pred,list_sigm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "runexp('np_10000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a742f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def necessary_budget(setup_file,error_value):\n",
    "    \n",
    "    '''\n",
    "    takes a str: setup file as argument and compare the methods based on this setup to find how much redundany is needed \n",
    "    to achieve a same value of error for each model\n",
    "    '''\n",
    "\n",
    "    params=load_experiment_setup(setup_file) #open setup file\n",
    "    np.random.seed(params[7])\n",
    "    \n",
    "    models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(), \n",
    "          \"conservative\":OneShotConservative(),\n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          \"10shot\":KShot(greedyness=0.1)\n",
    "         }\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "    \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2]) #choice of general parameters\n",
    "    sig_distr=params[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "    points=params[5]\n",
    "    sigmas = params[6]\n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "        elem._sigma=sigmas[k]\n",
    "    \n",
    "    list_redond=[]\n",
    "    for model_name, m in models.items():\n",
    "        metrics_value=100\n",
    "        redundancy=2\n",
    "        while metrics_value>error_value:\n",
    "            results=[]\n",
    "            redundancy+=1\n",
    "            exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "            repeats=50\n",
    "            result = {\"model\": model_name}\n",
    "            for i in range(repeats):\n",
    "                exp.reset()\n",
    "                this_run = m.run(exp)\n",
    "                result[\"iteration\"] = i\n",
    "                for metric_name, metric in metrics.items():\n",
    "                    result[\"metric\"] = metric_name\n",
    "                    result[\"value\"] = metric(exp, this_run)\n",
    "                    results.append(result.copy())\n",
    "            metrics_value=np.mean(np.array([results[i]['value'] for i in range(0,len(results),2)]))\n",
    "        list_redond.append(redundancy)\n",
    "        print(metrics_value)\n",
    "    #plt.figure()\n",
    "    #plt.title(\"budget necessary for each model to have error below threshold\")\n",
    "    #plt.bar(list(models.keys()),list_redond,color=['r','g','b'])\n",
    "    #plt.show()\n",
    "    \n",
    "    sn.set_style(\"whitegrid\")  # Utilisation d'un style de grille blanche pour améliorer la lisibilité\n",
    "    sn.barplot(x=list(models.keys()), y=list_redond, palette=\"viridis\")\n",
    "\n",
    "    # Ajouter des étiquettes pour le titre et les axes\n",
    "    plt.title(\"Necessary budget to obtain error below threshold of \" + str(error_value))\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Necessary budget (redundancy)\")\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the redundance value necessary to go below a given value of error for each model\n",
    "necessary_budget('np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def necessary_budget_continuous(setup_file):\n",
    "    \n",
    "    '''\n",
    "    takes a str: setup file as argument and compare the methods based on this setup to find how much redundany is needed \n",
    "    to achieve a same value of error for each model and for te whole spectrum of error value\n",
    "    '''\n",
    "    \n",
    "\n",
    "    params=load_experiment_setup(setup_file) #open setup file\n",
    "    #np.random.seed(params[7])\n",
    "    \n",
    "    models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(),\n",
    "          #\"bayesian\":OneShotBayesian(),\n",
    "          \"conservative\":OneShotConservative(),\n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          \"10shot\":KShot(greedyness=0.1)\n",
    "         }\n",
    "    name_list=list(models.keys())\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "    \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2]) #choice of general parameters\n",
    "    sig_distr=params[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "    points=params[5]\n",
    "    sigmas = params[6]\n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "        elem._sigma=sigmas[k]\n",
    "    \n",
    "\n",
    "    model_curves=[]\n",
    "    for model_name, m in models.items():\n",
    "        print(model_name)\n",
    "        error_list=[]\n",
    "        for redundancy in range(2,15):\n",
    "            results=[]\n",
    "            exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "            repeats=30\n",
    "            result = {\"model\": model_name}\n",
    "            for i in range(repeats):\n",
    "                exp.reset()\n",
    "                this_run = m.run(exp)\n",
    "                result[\"iteration\"] = i\n",
    "                for metric_name, metric in metrics.items():\n",
    "                    result[\"metric\"] = metric_name\n",
    "                    result[\"value\"] = metric(exp, this_run)\n",
    "                    results.append(result.copy())\n",
    "            metrics_value=np.mean(np.array([results[i]['value'] for i in range(0,len(results),2)]))\n",
    "            error_list.append(metrics_value)\n",
    "        model_curves.append(([k for k in range(2,15)],np.array([np.log10(k) for k in error_list])))\n",
    "            \n",
    "    fig=plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for k,elem in enumerate(model_curves):\n",
    "        ax.plot(elem[1],elem[0],label=name_list[k])\n",
    "   \n",
    "    ax.set_xlabel(\"log error values\") \n",
    "    \n",
    "    \n",
    "    #inversed axis\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('right')\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.set_ylabel(\"redundancy\") \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_budget_continuous('np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64398bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"nb_points\": list_param[0],\n",
    "\"nb_annotators\": list_param[1],\n",
    "\"redundancy\":list_param[2],\n",
    "\"sigma_distrib\": list_param[3],\n",
    "\"point_distrib\": list_param[4],\n",
    "\"points\": points,\n",
    "\"sigmas\": list_true_sig,\n",
    "\"random_seed\": 1234\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a single experiments (one set of params) and get the true pos, true sigmas, and predicted pos, predicted sigmas\n",
    "\n",
    "def run_exp_from_setup(setup,models):\n",
    "    \n",
    "    '''\n",
    "    takes the setup as a list as input and a dictionnary containing one model {name:model}\n",
    "    return [true_point,true_sig,pred_points,pred_sigms]\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(setup[7]) #random seed set for determinism\n",
    "    n_points,n_annotators,redundancy=(setup[0],setup[1],setup[2]) #choice of general parameters\n",
    "    sig_distr=setup[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "   \n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    \n",
    "    point_distr=setup[4] #choice of point distrib\n",
    "    \n",
    "    points = setup[5]\n",
    "        \n",
    "    list_true_sig = setup[6] \n",
    "   \n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    \n",
    "    list_sigm_pred=[]\n",
    "    list_point_pred=[]\n",
    "      \n",
    "    for model_name, m in models.items():\n",
    "\n",
    "        #result = {\"model\": model_name}\n",
    "        exp.reset()\n",
    "        this_run = m.run(exp)\n",
    "        locations=this_run[\"locations\"]\n",
    "        sigmas=this_run[\"sigmas\"]\n",
    "        list_sigm_pred.append(sigmas)\n",
    "        list_point_pred.append(locations)\n",
    "        #for metric_name, metric in metrics.items():\n",
    "        #result[\"metric\"] = metric_name\n",
    "        #result[\"value\"] = metric(exp, this_run)\n",
    "        #results.append(result.copy())\n",
    "    return points,list_true_sig,list_point_pred,list_sigm_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696a4ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "setup1=load_experiment_setup('np_10000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl') #load the experiment setup as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=run_exp_from_setup(setup1,{\"10shot\":KShot(greedyness=0.1)})\n",
    "test2=run_exp_from_setup(setup1,{\"direct\":OneShotDirect()}) #run an experiment from setup\n",
    "#save_experiment(setup1,test)\n",
    "print(len(test))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"distribution of ground-truth points\")\n",
    "plt.hist(test[2])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"distribution of predicted points\")\n",
    "plt.hist(test2[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbd815",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=runexp(params) #launch the experiment\n",
    "#save_experiment(params,res) #save the experiment\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7824ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot and compare from a list of params\n",
    "def run_and_plot(list_setup_file):\n",
    "    \n",
    "    list_df=[]\n",
    "    list_duration=[]\n",
    "    for element in list_setup_file:\n",
    "        print(\"element:\",element)\n",
    "        models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(),\n",
    "          #\"bayesian\":OneShotBayesian(),\n",
    "          \"conservative\":OneShotConservative(),\n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          \"10shot\":KShot(greedyness=0.1)\n",
    "         }\n",
    "        metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "        \n",
    "        params=load_experiment_setup(element) #get the params for setup file k\n",
    "        \n",
    "        random.seed(params[7])\n",
    "        \n",
    "        n_points,n_annotators,redundancy=(params[0],params[1],params[2])\n",
    "        \n",
    "        #points = point_distribution.rvs(n_points)\n",
    "        points = params[5]\n",
    "        sigmas = params[6]\n",
    "        \n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1)) #here we decide the distribution of sigmas\n",
    "        \n",
    "        ann_set = annotator_population.sample(n_annotators) #not usefull because already have sigmas*\n",
    "        for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "            elem._sigma=sigmas[k]\n",
    "       \n",
    "        \n",
    "        start_time = time.time()\n",
    "        exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "        res = compare(models, exp, metrics, 30)\n",
    "        results=res[0] #get the results\n",
    "        #print(results)\n",
    "        durations=res[1]\n",
    "      \n",
    "        list_duration.append(durations)\n",
    "        df=pd.DataFrame(results)\n",
    "        list_df.append(df)\n",
    "        \n",
    "    #print(\"duration times:\",list_duration)\n",
    "    #create the box plot\n",
    "    fig, axs = plt.subplots(1,len(list_setup_file), figsize=(10, 5),sharey=True)\n",
    "    for i, elem in enumerate(list_df):\n",
    "        sn.boxplot(data=elem[elem[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs[i])\n",
    "        integers = re.findall(r'\\d+',list_setup_file[i])\n",
    "        int_tuple = tuple(int(k) for k in integers)\n",
    "        axs[i].set_title(str(int_tuple))\n",
    "        axs[i].set(xlabel=\"Model\", ylabel=\"Mean_location_norm_error\")\n",
    "       \n",
    "    #titre = input(\"Enter a title : \")\n",
    "    #fig.suptitle(titre)\n",
    "    #\"Constant number of points, varying number of annotators\"\n",
    "    \n",
    "    #create the distribution plot\n",
    "    fig2, axs2 = plt.subplots(1,len(list_setup_file), figsize=(15, 5),sharey=True,sharex=True)\n",
    "    for ax in axs2:\n",
    "        ax.grid()\n",
    "    for i,elem in enumerate(list_df):\n",
    "        plot_models = [\"mean\",\"direct\",\"iterative\",\"conservative\",\"conservative2\",\"10shot\"]\n",
    "        le = elem[elem[\"metric\"]==\"mean_location_norm_error\"]\n",
    "        for n in plot_models:\n",
    "            le1 = le[le[\"model\"]==n]\n",
    "            sn.kdeplot(le1[\"value\"],ax=axs2[i])\n",
    "            integers = re.findall(r'\\d+',list_setup_file[i])\n",
    "            int_tuple = tuple(int(k) for k in integers)\n",
    "            axs2[i].set_title(str(int_tuple))\n",
    "            axs2[i].set(xlabel=\"mean_location_norm_error\", ylabel=\"density\")\n",
    "\n",
    "        #fig.suptitle(titre)\n",
    "    plt.show()\n",
    "    #plt.legend(plot_models)\n",
    "    #plt.figure(\"2\")\n",
    "    #plt.title(\"duration for each model\")\n",
    "    #plt.plot(list_duration)\n",
    "    #plt.show()\n",
    "    return(list_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes te two setups and compare\n",
    "dur=run_and_plot(['np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl','np_10000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl','np_100000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl'])\n",
    "print(dur)\n",
    "plt.figure()\n",
    "list_label=[\"mean\",\"direct\",\"iterative\",\"conservative\",\"conservative2\",\"10shot\"]\n",
    "for k in range(len(dur[0])):\n",
    "    plt.plot([dur[0][k],dur[1][k],dur[2][k]],label=list_label[k])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot and compare from a list of params\n",
    "def greedyness_impact(setup_file):\n",
    "     \n",
    "    list_df=[]\n",
    "    models = {\n",
    "          \"greed=0.001\":KShot(greedyness=0.1),\n",
    "          \"greed=0.01\":KShot(greedyness=0.2),\n",
    "          \"greed=0.1\":KShot(greedyness=0.3),\n",
    "          \"greed=1\":KShot(greedyness=0.4),\n",
    "          \"greed=10\":KShot(greedyness=0.5)   \n",
    "         }\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "        \n",
    "    params=load_experiment_setup(setup_file) #get the params from the setup file\n",
    "        \n",
    "    random.seed(params[7])\n",
    "        \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2])\n",
    "        \n",
    "    #points = point_distribution.rvs(n_points)\n",
    "    points = params[5]   \n",
    "    sigmas = params[6]\n",
    "        \n",
    "    annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1)) #here we decide the distribution of sigmas\n",
    "        \n",
    "    ann_set = annotator_population.sample(n_annotators) #not usefull because already have sigmas*\n",
    "    for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "        elem._sigma=sigmas[k]\n",
    "       \n",
    "    \n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    results = compare(models, exp, metrics, 100)[0]\n",
    "    df=pd.DataFrame(results)\n",
    "    list_df.append(df)\n",
    " \n",
    "    #create the box plot\n",
    "    fig, axs = plt.subplots(1,1, figsize=(10, 5),sharey=True)\n",
    "    for i, elem in enumerate(list_df):\n",
    "        sn.boxplot(data=elem[elem[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs)\n",
    "    \n",
    "        #axs.set_title(str(int_tuple))\n",
    "        axs.set(xlabel=\"Model\", ylabel=\"Mean_location_norm_error\")\n",
    "       \n",
    "    #titre = input(\"Enter a title : \")\n",
    "    #fig.suptitle(titre)\n",
    "    #\"Constant number of points, varying number of annotators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedyness_impact(\"np_10000_na_50_rd_7_sd_uniform_pd_uniform_setup.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities):\n",
    "    \"\"\"Calculate the entropy of a probability distribution.\n",
    "\n",
    "    Arguments:\n",
    "    probabilities -- a list or array of probabilities that sum to 1\n",
    "\n",
    "    Returns:\n",
    "    The entropy of the probability distribution, in bits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the entropy\n",
    "    h = 0.0\n",
    "    for p in probabilities:\n",
    "        if p > 0:\n",
    "            h -= p * np.log2(p)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b011d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot and compare from a list of params\n",
    "def entropy_plot(setup_file):\n",
    "\n",
    "    params=load_experiment_setup(setup_file) #get the params from the setup file\n",
    "    random.seed(params[7])\n",
    "    #points = point_distribution.rvs(n_points)\n",
    "    points = params[5]   \n",
    "    sigmas = params[6]\n",
    "    \n",
    "    list_entropy=[]\n",
    "    greedyness_list=np.array([0.001,0.01,0.1,1,5,10,20,30,100])\n",
    "    list_entropy=[entropy(softmax_stable(k*np.array(sigmas))) for k in greedyness_list]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"entropy of the distribution of points depending on greedyness\")\n",
    "    plt.plot(np.log10(greedyness_list),list_entropy)\n",
    "    plt.xlabel(\"greedyness\")\n",
    "    plt.ylabel(\"entropy\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_plot(\"np_10000_na_50_rd_7_sd_uniform_pd_uniform_setup.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03238c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot and compare from a list of params\n",
    "def impact_of_distrib(setup_file):\n",
    "    \n",
    "    \n",
    "\n",
    " \n",
    "    models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(), \n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          \"conservative\":OneShotConservative(),\n",
    "          \"10shot\":KShot(greedyness=0.1)\n",
    "         }\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "        \n",
    "    params=load_experiment_setup(setup_file) #get the params from the setup file\n",
    "        \n",
    "    random.seed(params[7])\n",
    "        \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2])\n",
    "        \n",
    "    #points = point_distribution.rvs(n_points)\n",
    "    \n",
    "    sigmas = params[6]\n",
    "        \n",
    "    annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1)) #here we decide the distribution of sigmas\n",
    "        \n",
    "    ann_set = annotator_population.sample(n_annotators) #not usefull because already have sigmas*\n",
    "    for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "        elem._sigma=sigmas[k]\n",
    "       \n",
    "    \n",
    "    #test1: uniform\n",
    "    list_df1=[]\n",
    "    points = params[5]\n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    results = compare(models, exp, metrics, 50)[0]\n",
    "    df1=pd.DataFrame(results)\n",
    "   \n",
    "    \n",
    "    #test2: tokyo\n",
    "    points = tok(n_points)\n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    results = compare(models, exp, metrics, 50)[0]\n",
    "    df2=pd.DataFrame(results)\n",
    "    for i in range(len(df2[\"model\"])):\n",
    "        df2.loc[i, \"model\"] = df2.loc[i, \"model\"] + \"1\"\n",
    "    df_concat = pd.concat([df1, df2])\n",
    "    \n",
    "    \n",
    "    #create the box plot\n",
    "    fig, axs = plt.subplots(1,1, figsize=(10, 5),sharey=True)\n",
    "    sn.boxplot(data=df_concat[df_concat[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs)\n",
    "    \n",
    "    \n",
    "    axs.set_title(\"a\")\n",
    "    axs.set(xlabel=\"Model\", ylabel=\"Mean_location_norm_error\")\n",
    "    \n",
    "    #fig2, axs2 = plt.subplots(1,1, figsize=(5, 5),sharey=True)\n",
    "    #sn.boxplot(data=df2[df2[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs2)\n",
    "    \n",
    "    \n",
    "    #axs2.set_title(\"a\")\n",
    "    #axs2.set(xlabel=\"Model\", ylabel=\"Mean_location_norm_error\")\n",
    "       \n",
    "    #titre = input(\"Enter a title : \")\n",
    "    #fig.suptitle(titre)\n",
    "    #\"Constant number of points, varying number of annotators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_of_distrib('np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters are defined as a list of this shape:\n",
    "\n",
    "#param=[n_points,n_annotators,redundancy,sigma_distrib,point_distrib,model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ab067",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# saving experiments setup\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_experiment_setup(params):\n",
    "    # Create a dictionary containing experiment data\n",
    "    \n",
    "    np.random.seed(1234) #we set a seed to generate each time the same points/sigmas\n",
    "    random.seed(1234)\n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2]) #choice of general parameters\n",
    "    sig_distr=params[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "    point_distr=params[4] #choice of point distrib\n",
    "    \n",
    "    if point_distr=='uniform':\n",
    "        point_distribution = uniform()\n",
    "        points = point_distribution.rvs(n_points)\n",
    "    else:\n",
    "        points =  tok(n_points)\n",
    "        \n",
    "    list_tru_sig=[]\n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    list_true_sig=[ann_set.annotators[k]._sigma for k in range(len(ann_set.annotators))]\n",
    "    #print(list_true_sig)\n",
    "    experiment_data = {\n",
    "        \"nb_points\": params[0],\n",
    "        \"nb_annotators\": params[1],\n",
    "        \"redundancy\": params[2],\n",
    "        \"sigma_distrib\": params[3],\n",
    "        \"point_distrib\": params[4],\n",
    "        \"points\": points,\n",
    "        \"sigmas\": list_true_sig,\n",
    "        \"random_seed\": np.random.randint(0,10000)\n",
    "    }\n",
    "    \n",
    "    filename = f\"np_{params[0]}_na_{params[1]}_rd_{params[2]}_sd_{params[3]}_pd_{params[4]}_setup.pkl\"\n",
    "    # Save the experiment data to a file using pickle\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(experiment_data, f, protocol=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe600136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save an experiment setup in a pickle file along with a random seed\n",
    "save_experiment_setup([1000,50,3,'uniform','tokyo'])\n",
    "save_experiment_setup([10000,50,3,'uniform','tokyo'])\n",
    "save_experiment_setup([100000,50,3,'uniform','uniform'])\n",
    "\n",
    "save_experiment_setup([1000,50,5,'uniform','uniform'])\n",
    "save_experiment_setup([10000,50,5,'uniform','uniform'])\n",
    "save_experiment_setup([100000,50,5,'uniform','uniform'])\n",
    "\n",
    "save_experiment_setup([1000,50,7,'uniform','uniform'])\n",
    "save_experiment_setup([10000,50,7,'uniform','uniform'])\n",
    "save_experiment_setup([100000,50,7,'uniform','uniform'])\n",
    "\n",
    "save_experiment_setup([1000,50,15,'uniform','uniform'])\n",
    "save_experiment_setup([10000,50,15,'uniform','uniform'])\n",
    "save_experiment_setup([100000,50,15,'uniform','uniform'])\n",
    "\n",
    "\n",
    "\n",
    "save_experiment_setup([1000,30,3,'uniform','uniform'])\n",
    "save_experiment_setup([10000,30,3,'uniform','uniform'])\n",
    "save_experiment_setup([100000,30,3,'uniform','uniform'])\n",
    "\n",
    "save_experiment_setup([100,30,3,'uniform','uniform'])\n",
    "save_experiment_setup([100,30,5,'uniform','uniform'])\n",
    "save_experiment_setup([100,30,7,'uniform','uniform'])\n",
    "\n",
    "save_experiment_setup([1000,100,3,'uniform','uniform'])\n",
    "save_experiment_setup([10000,100,3,'uniform','uniform'])\n",
    "save_experiment_setup([100000,100,3,'uniform','uniform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affced96",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# saving experiments setup (only the setup, not te results)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_experiment(list_param,results):\n",
    "    # Create a dictionary containing experiment data\n",
    "    experiment_data = {\n",
    "        \"nb_points\": list_param[0],\n",
    "        \"nb_annotators\": list_param[1],\n",
    "        \"redundancy\":list_param[2],\n",
    "        \"sigma_distrib\": list_param[3],\n",
    "        \"point_distrib\": list_param[4],\n",
    "        \"points\": results[0],\n",
    "        \"sigmas\": results[1],\n",
    "        \"method_results\": {\n",
    "                \"name\": list_param[5],\n",
    "                \"points\":results[2][0],\n",
    "                \"sigmas\":results[3][0]\n",
    "            }\n",
    "    }\n",
    "            \n",
    "            #,\n",
    "            #\"conservative2\": {\n",
    "            #    \"points\":results[2][1] ,\n",
    "            #    \"sigmas\": results[3][1]\n",
    "            #},\n",
    "            # \"10shot\": {\n",
    "            #    \"points\": results[2][2],\n",
    "            #    \"sigmas\": results[3][2]\n",
    "           # }\n",
    "        #}\n",
    "      #  }\n",
    "    \n",
    "    filename = f\"np_{list_param[0]}_na_{list_param[1]}_rd_{list_param[2]}_sd_{list_param[3]}_pd_{list_param[4]}.pkl\"\n",
    "    # Save the experiment data to a file using pickle\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(experiment_data, f, protocol=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13606360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = os.path.join('..','..', \"np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl\")\n",
    "\n",
    "setup1=load_experiment_setup('np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl')\n",
    "setup2=load_experiment_setup('np_1000_na_10_rd_3_sd_uniform_pd_uniform_setup.pkl')\n",
    "#print(setup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(setup1[6][0:20]) #making sure that the points distributions are the same for two different experiment setups\n",
    "print(setup2[6][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e3f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2cc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
