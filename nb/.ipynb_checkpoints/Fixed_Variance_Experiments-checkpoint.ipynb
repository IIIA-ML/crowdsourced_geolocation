{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from crowdgeoloc.one_d import sample_tokyo_latitudes, generate_annotators, annotate, \\\n",
    "FunctionNormalAnnotatorPopulation, SimpleNormalAnnotatorPopulation\n",
    "from crowdgeoloc.experiment import ActiveAnnotationContest, mean_location_norm_error, mean_sigma_norm_error, \\\n",
    "    softmax_stable, load_experiment, load_experiment_setup, save_experiment, save_experiment_setup\n",
    "from crowdgeoloc.fixed import OneShotDirect,OneShotBayesian, OneShotConservative, OneShotConservative2, OneShotIterative, \\\n",
    "                                OneShotMean, KShot\n",
    "from crowdgeoloc.fixed import OneShotDirect,OneShotBayesian, OneShotConservative, OneShotConservative2, OneShotIterative, \\\n",
    "                                OneShotMean, KShot\n",
    "from crowdgeoloc.spatial import OneShotSpatialBayesian, KShotSpatial\n",
    "import seaborn\n",
    "from seaborn import color_palette\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, uniform\n",
    "import cmdstanpy as cmd\n",
    "import seaborn as sn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSTANT VARIANCE PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the setup files for the constant variance experiments\n",
    "save_experiment_setup([100000,50,3,'uniform','uniform'])\n",
    "save_experiment_setup([50000,50,3,'uniform','uniform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok(nb_points):\n",
    "    '''function to create the tokyo distribution array'''\n",
    "    tab=sample_tokyo_latitudes(nb_points)\n",
    "    tab=(tab-np.min(tab))/(np.max(tab)-np.min(tab))\n",
    "    return(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare(models, exp, metrics, repeats):\n",
    "    '''\n",
    "    function to compare models by running the experiments the number of repeats given and evaluate them using metrics provided\n",
    "    '''\n",
    "    results = []\n",
    "    duration_list=[]\n",
    "    for model_name, m in models.items():\n",
    "        #print(model_name)\n",
    "        result = {\"model\": model_name}\n",
    "        start_time = time.time()\n",
    "        for i in range(repeats):\n",
    "            exp.reset()\n",
    "            this_run = m.run(exp)\n",
    "            result[\"iteration\"] = i\n",
    "            for metric_name, metric in metrics.items():\n",
    "                result[\"metric\"] = metric_name\n",
    "                result[\"value\"] = metric(exp, this_run)\n",
    "                results.append(result.copy())\n",
    "        end_time=time.time()\n",
    "        duration=end_time-start_time\n",
    "        duration_list.append(duration)\n",
    "    return results,duration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runexp(setup_file):\n",
    "    \n",
    "    '''\n",
    "    takes a str: setup file name as argument and compare the methods based on this setup\n",
    "    '''\n",
    "\n",
    "    params=load_experiment_setup(setup_file) #open setup file\n",
    "    np.random.seed(params[7])\n",
    "    \n",
    "    models = {#\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          #\"iterative\":OneShotIterative(), \n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          #\"conservative\":OneShotConservative(),\n",
    "          \"10shot\":KShot(greediness=1.2)\n",
    "         }\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "    \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2]) #choice of general parameters\n",
    "    sig_distr=params[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "    point_distr=params[4] #choice of point distrib\n",
    "    \n",
    "    if point_distr=='uniform':\n",
    "        point_distribution = uniform()\n",
    "        points = point_distribution.rvs(n_points)\n",
    "    else:\n",
    "        points =  tok(n_points)\n",
    "        \n",
    "    list_tru_sig=[]\n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    list_true_sig=[ann_set.annotators[k]._sigma for k in range(len(ann_set.annotators))]\n",
    "    \n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    list_sigm_pred=[]\n",
    "    list_point_pred=[]\n",
    "    for model_name, m in models.items():\n",
    "        print(model_name)\n",
    "        result = {\"model\": model_name}\n",
    "        exp.reset()\n",
    "        this_run = m.run(exp)\n",
    "        locations=this_run[\"locations\"]\n",
    "        sigmas=this_run[\"sigmas\"]\n",
    "        list_sigm_pred.append(sigmas)\n",
    "        list_point_pred.append(locations)\n",
    "    return points,list_true_sig,list_point_pred,list_sigm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_and_plot(list_setup_file):\n",
    "    '''\n",
    "    takes a list of setup files as arguments and plot the metrics bar chart for each of the setups, return the list of durations of the different methods \n",
    "    '''\n",
    "    \n",
    "    list_df=[]\n",
    "    list_duration=[]\n",
    "    for element in list_setup_file:\n",
    "        print(\"element:\",element)\n",
    "        models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(),\n",
    "          \"conserv\":OneShotConservative(),\n",
    "          \"conserv2\":OneShotConservative2(),\n",
    "          #\"bayesian\":OneShotBayesian(),\n",
    "          \"10shot\":KShot(greediness=1.1)\n",
    "         }\n",
    "        \n",
    "        name_list=list(models.keys())\n",
    "        metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "        \n",
    "        params=load_experiment_setup(element)\n",
    "        \n",
    "        random.seed(params[7])\n",
    "        \n",
    "        n_points,n_annotators,redundancy=(params[0],params[1],params[2])\n",
    "        \n",
    "        points = params[5]\n",
    "        sigmas = params[6]\n",
    "        \n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "        \n",
    "        ann_set = annotator_population.sample(n_annotators) \n",
    "        for k,elem in enumerate(ann_set.annotators):\n",
    "            elem._sigma=sigmas[k]\n",
    "       \n",
    "        \n",
    "        exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "        res = compare(models, exp, metrics, 100)\n",
    "        results=res[0] \n",
    "        durations=res[1]\n",
    "      \n",
    "        list_duration.append(durations)\n",
    "        df=pd.DataFrame(results)\n",
    "        list_df.append(df)\n",
    "        #print(results)\n",
    "        \n",
    "\n",
    "    #create the box plot\n",
    "    fig, axs = plt.subplots(1,len(list_setup_file), figsize=(10, 5),sharey=True)\n",
    "    for i, elem in enumerate(list_df):\n",
    "        sn.boxplot(data=elem[elem[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs[i])  #ATTENTION #[i])\n",
    "        integers = re.findall(r'\\d+',list_setup_file[i])\n",
    "        int_tuple = tuple(int(k) for k in integers)\n",
    "        axs[i].set_title(str(int_tuple))\n",
    "        axs[i].set(xlabel=\"Model\", ylabel=\"Mean location norm error\") #ATTENTION axi[i]\n",
    "        axs[i].set_xticklabels(name_list,rotation=45, ha='right') #axs[i].get_xticks()\n",
    "       \n",
    " \n",
    "    return(list_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple on how to run the run_and_plot function\n",
    "run_and_plot(['np_1000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl','np_10000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl','np_100000_na_50_rd_3_sd_uniform_pd_uniform_setup.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def necessary_budget_continuous(setup_file):\n",
    "    \n",
    "    '''\n",
    "    takes a str: setup file as argument and compare the methods based on this setup to find how much redundany is needed \n",
    "    to achieve a same value of error for each model, then plot the results\n",
    "    '''\n",
    "    \n",
    "\n",
    "    params=load_experiment_setup(setup_file) #open setup file\n",
    "    #np.random.seed(params[7])\n",
    "    \n",
    "    models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(),\n",
    "          #\"bayesian\":OneShotBayesian(),\n",
    "          \"conservative\":OneShotConservative(),\n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          \"10shot-1.1\":KShot(greediness=1.1),\n",
    "          \"10shot-1.4\":KShot(greediness=1.4)\n",
    "         }\n",
    "    name_list=list(models.keys())\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error}\n",
    "    #\"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "    \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2]) #choice of general parameters\n",
    "    sig_distr=params[3] #choice of the sigma distrib\n",
    "    if sig_distr=='uniform':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1))\n",
    "    if sig_distr=='beta':\n",
    "        annotator_population = SimpleNormalAnnotatorPopulation()\n",
    "    \n",
    "    points=params[5]\n",
    "    sigmas = params[6]\n",
    "    ann_set = annotator_population.sample(n_annotators)\n",
    "    for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "        elem._sigma=sigmas[k]\n",
    "    \n",
    "\n",
    "    model_curves=[]\n",
    "    for model_name, m in models.items():\n",
    "        print(model_name)\n",
    "        error_list=[]\n",
    "        for redundancy in range(2,40):\n",
    "            results=[]\n",
    "            exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "            repeats=60\n",
    "            result = {\"model\": model_name}\n",
    "            for i in range(repeats):\n",
    "                exp.reset()\n",
    "                this_run = m.run(exp)\n",
    "                result[\"iteration\"] = i\n",
    "                for metric_name, metric in metrics.items():\n",
    "                    result[\"metric\"] = metric_name\n",
    "                    result[\"value\"] = metric(exp, this_run)\n",
    "                    results.append(result.copy())\n",
    "                \n",
    "        \n",
    "            metrics_value=np.mean(np.array([results[i]['value'] for i in range(0,len(results),2)]))\n",
    "            error_list.append(metrics_value)\n",
    "     \n",
    "        model_curves.append([[k for k in range(2,40)],np.array([k for k in error_list])])\n",
    "        \n",
    "            \n",
    "    fig=plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for k,elem in enumerate(model_curves):\n",
    "        if name_list[k]==\"10shot-1.1\" or name_list[k]==\"10shot-1.4\" :\n",
    "            ax.plot(elem[1],elem[0],label=name_list[k])\n",
    "        else:\n",
    "            ax.plot(elem[1],elem[0],label=name_list[k],linestyle=\"dashed\")\n",
    "        \n",
    "   \n",
    "    ax.set_xlabel(\"error values\") \n",
    "    \n",
    "    \n",
    "    #inversed axis\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('right')\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.set_ylabel(\"redundancy\") \n",
    "\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedyness_impact(setup_file,greediness_list):\n",
    "    '''\n",
    "    takes a list of greediness to test and a setup file and plot the location error depending on greediness\n",
    "    '''\n",
    "     \n",
    "    list_df=[]\n",
    "    models = {}\n",
    "    for greediness in greediness_list:\n",
    "        models[f\"greed={greediness}\"] = KShot(greediness=greediness)\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error}\n",
    "    \n",
    "    name_list=list(models.keys())\n",
    "    new_labels=[k[6:] for k in name_list]\n",
    "    print(name_list)\n",
    "    \n",
    "    params=load_experiment_setup(setup_file) #get the params from the setup file\n",
    "        \n",
    "    random.seed(params[7])\n",
    "        \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2])\n",
    "        \n",
    "    #points = point_distribution.rvs(n_points)\n",
    "    points = params[5]   \n",
    "    sigmas = params[6]\n",
    "        \n",
    "    annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1)) #here we decide the distribution of sigmas\n",
    "        \n",
    "    ann_set = annotator_population.sample(n_annotators) #not usefull because already have sigmas*\n",
    "    for k,elem in enumerate(ann_set.annotators): #we put the setup sigmas\n",
    "        elem._sigma=sigmas[k]\n",
    "       \n",
    "    \n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    results = compare(models, exp, metrics, 50)[0]\n",
    "    df=pd.DataFrame(results)\n",
    "    list_df.append(df)\n",
    " \n",
    "    #create the box plot\n",
    "    fig, axs = plt.subplots(1,1, figsize=(10, 5),sharey=True)\n",
    "    for i, elem in enumerate(list_df):\n",
    "        sn.boxplot(data=elem[elem[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs,boxprops=dict(facecolor='none', edgecolor='black', linewidth=1.5))\n",
    "    \n",
    "        #axs.set_title(str(int_tuple))\n",
    "        axs.set(xlabel=\"greedyness\", ylabel=\"Mean_location_norm_error\")\n",
    "        axs.set_xticklabels(new_labels)\n",
    "       \n",
    "    #titre = input(\"Enter a title : \")\n",
    "    #fig.suptitle(titre)\n",
    "    #\"Constant number of points, varying number of annotators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impact_of_distrib(setup_file):\n",
    "    ''' \n",
    "    plot the comparison of the boxplots of the metrics in a uniform vs tokyo distribution\n",
    "    '''\n",
    " \n",
    "    models = {\"mean\": OneShotMean(),\n",
    "          \"direct\":OneShotDirect(), \n",
    "          \"iterative\":OneShotIterative(), \n",
    "          \"conservative\":OneShotConservative(),\n",
    "          \"conservative2\":OneShotConservative2(),\n",
    "          \"10shot\":KShot(greediness=1.1)\n",
    "         }\n",
    "    metrics = {\"mean_location_norm_error\":mean_location_norm_error,\n",
    "          \"mean_sigma_norm_error\":mean_sigma_norm_error}\n",
    "        \n",
    "    params=load_experiment_setup(setup_file) \n",
    "        \n",
    "    random.seed(params[7])\n",
    "        \n",
    "    n_points,n_annotators,redundancy=(params[0],params[1],params[2])\n",
    "        \n",
    "    \n",
    "    sigmas = params[6]\n",
    "        \n",
    "    annotator_population = SimpleNormalAnnotatorPopulation(uniform(scale=0.1)) \n",
    "        \n",
    "    ann_set = annotator_population.sample(n_annotators) \n",
    "    for k,elem in enumerate(ann_set.annotators):\n",
    "        elem._sigma=sigmas[k]\n",
    "       \n",
    "    \n",
    "    #test1: uniform\n",
    "    list_df1=[]\n",
    "    points = params[5]\n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    results = compare(models, exp, metrics, 30)[0] \n",
    "    df1=pd.DataFrame(results)\n",
    "   \n",
    "    \n",
    "    #test2: tokyo\n",
    "    points = tok(n_points)\n",
    "    exp = ActiveAnnotationContest(points, ann_set, max_total_annotations=n_points*redundancy)\n",
    "    results = compare(models, exp, metrics, 30)[0] \n",
    "    df2=pd.DataFrame(results)\n",
    "    \n",
    "    \n",
    "    for i in range(len(df2[\"model\"])):\n",
    "        df2.loc[i, \"model\"] = df2.loc[i, \"model\"] + \"1\"\n",
    "    df_concat = pd.concat([df1, df2])\n",
    "    \n",
    "    \n",
    "    #create the box plot\n",
    "    fig, axs = plt.subplots(1,1, figsize=(10, 5),sharey=True)\n",
    "    new_labels = [\"mean\", \"direct\", \"iter\",\"cons\",\"cons2\",\"mean\", \"direct\", \"iter\",\"cons\",\"cons2\"]\n",
    "    new_colors = seaborn.color_palette(\"flare\", n_colors=len(new_labels))\n",
    "    median_vals = df_concat.groupby(\"model\")[\"value\"].mean().sort_values()\n",
    "    sn.boxplot(data=df_concat[df_concat[\"metric\"]==\"mean_location_norm_error\"], x=\"model\", y=\"value\", ax=axs,palette=dict(zip(median_vals.index, new_colors)))\n",
    "    \n",
    "    axs.set_xticklabels(new_labels)\n",
    "\n",
    "    # Add text\n",
    "    axs.text(0.2, 0.9, 'Uniform', transform=axs.transAxes, fontsize=14, verticalalignment='top')\n",
    "    axs.text(0.7, 0.9, 'Tokyo', transform=axs.transAxes, fontsize=14, verticalalignment='top')\n",
    "\n",
    "    \n",
    "    x_middle = len(df_concat[\"model\"].unique()) / 2 \n",
    "    axs.axvline(x=x_middle-0.5, color='r', linestyle='--')\n",
    "\n",
    "    axs.set(xlabel=\"Model\", ylabel=\"Mean location norm error\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
